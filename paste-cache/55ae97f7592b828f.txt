                             │   1774 │   │   else:                                                       │
                             │ ❱ 1775 │   │   │   return self._call_impl(*args, **kwargs)                 │
                             │   1776 │                                                                   │
                             │   1777 │   # torchrec tests the code consistency with the following code   │
                             │   1778 │   # fmt: off                                                      │
                             │                                                                            │
                             │ /home/fishhouses/.local/share/uv/tools/headwater-server/lib/python3.13/sit │
                             │ e-packages/torch/nn/modules/module.py:1786 in _call_impl                   │
                             │                                                                            │
                             │   1783 │   │   if not (self._backward_hooks or self._backward_pre_hooks or │
                             │        or self._forward_pre_hooks                                          │
                             │   1784 │   │   │   │   or _global_backward_pre_hooks or _global_backward_h │
                             │   1785 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hoo │
                             │ ❱ 1786 │   │   │   return forward_call(*args, **kwargs)                    │
                             │   1787 │   │                                                               │
                             │   1788 │   │   result = None                                               │
                             │   1789 │   │   called_always_called_hooks = set()                          │
                             │                                                                            │
                             │ /home/fishhouses/.local/share/uv/tools/headwater-server/lib/python3.13/sit │
                             │ e-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:784 in  │
                             │ forward                                                                    │
                             │                                                                            │
                             │    781 │   │   if token_type_ids is None:                                  │
                             │    782 │   │   │   if hasattr(self.embeddings, "token_type_ids"):          │
                             │    783 │   │   │   │   buffered_token_type_ids = self.embeddings.token_typ │
                             │ ❱  784 │   │   │   │   buffered_token_type_ids_expanded =                  │
                             │        buffered_token_type_ids.expand(batch_size, seq_length)              │
                             │    785 │   │   │   │   token_type_ids = buffered_token_type_ids_expanded   │
                             │    786 │   │   │   else:                                                   │
                             │    787 │   │   │   │   token_type_ids = torch.zeros(input_shape, dtype=tor │
                             │        device=device)                                                      │
                             ╰────────────────────────────────────────────────────────────────────────────╯
                             RuntimeError: The expanded size of the tensor (552) must match the existing
                             size (514) at non-singleton dimension 1.  Target sizes: [1, 552].  Tensor
                             sizes: [1, 514]
