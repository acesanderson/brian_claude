# Course Catalog Scraper Skill

## Purpose
Scrape and analyze course catalogs from training providers (HubSpot Academy, Salesforce Trailhead, Anaconda Learning, etc.) to evaluate content for potential LinkedIn Learning licensing. Generate standardized course catalog CSV files for comparison and analysis.

## When to Use
Invoke when user requests:
- "Scrape [provider] course catalog"
- "Generate course list from [URL]"
- "Analyze [provider]'s training offerings"
- "Create catalog for [provider]"

## Input Parameters
- **Provider Name**: Name of the training platform (e.g., "HubSpot Academy")
- **URL**: Starting URL for the course catalog/listing page
- **Optional**: Credentials if provided (for email-gated content)

## Workflow

### Phase 1: Discovery & Reconnaissance (5-10 minutes)

**Objective**: Understand the site architecture before attempting extraction.

1. **Initial Page Analysis**
   - Fetch the URL and examine the HTML structure
   - Use WebFetch to understand page organization and content type
   - Document findings: "This is a [single page / paginated / navigation-based] catalog"

2. **Data Source Detection** - Check for (in order of preference):
   - **JSON in hidden inputs** (like `<input id="archive-posts">`)
   - **JavaScript data variables** (search for `var courses =` or `window.__INITIAL_STATE__`)
   - **API endpoints** (inspect Network tab patterns, look for `/api/courses` or similar)
   - **Structured data** (Schema.org JSON-LD in `<script type="application/ld+json">`)
   - **Server-rendered HTML** (course cards, list items with consistent structure)

3. **Architecture Mapping**
   - **Single Page**: All courses on one page (simple extraction)
   - **Paginated**: Multiple pages with page numbers or "Next" buttons
   - **Infinite Scroll**: Lazy-loaded content (requires Selenium or API detection)
   - **Navigation-based**: Must visit topic/category pages first (like AMA)
   - **Search/Filter-based**: Courses behind search interface

4. **Obstacle Identification**
   - Email gates / signup walls
   - Authentication requirements
   - Rate limiting
   - CAPTCHA
   - JavaScript-heavy (content not in initial HTML)

5. **Field Mapping** - Identify available metadata:
   - Required: Title, URL
   - Preferred: Description, Duration, Level, Price, Format
   - Optional: Instructor, Prerequisites, Learning Objectives, Reviews

### Phase 2: Extraction Strategy

Based on discovery, choose the appropriate approach:

#### Strategy A: JSON Extraction (Fastest)
If JSON data is embedded or API endpoint found:
```python
# Extract JSON from hidden input or API
# Parse and transform to standard format
# No need for complex scraping
```

#### Strategy B: Static HTML Scraping (Most Common)
If courses are server-rendered:
```python
# Use BeautifulSoup
# Find course card/list elements
# Extract metadata from consistent structure
# Handle pagination with requests loop
```

#### Strategy C: Browser Automation (Lazy Loading)
If content loads dynamically:
```python
# Use Selenium (note: requires setup)
# Scroll to trigger lazy loading
# Extract rendered content
# More resource-intensive
```

#### Strategy D: Navigation Crawl (Multi-level)
If courses organized by categories:
```python
# Get category/topic list first
# Visit each category page
# Extract courses from each
# Aggregate results
```

#### Strategy E: Manual Documentation (Blocked)
If email gate or auth required:
```python
# Document the obstacle
# Provide manual inspection findings
# Recommend obtaining credentials
# Note what's visible pre-auth
```

### Phase 3: Implementation

1. **Create Python Script** in working directory:
   - Name: `scrape_{provider_slug}.py`
   - Include imports: requests, bs4, csv, json
   - Implement chosen strategy
   - Add error handling and rate limiting (sleep between requests)

2. **Extract Course Data**
   - Run scraper
   - Show progress (X of Y courses scraped)
   - Handle errors gracefully
   - Note any limitations encountered

3. **Validate Data Quality**
   - Check for missing critical fields
   - Verify URLs are valid
   - Count total courses found
   - Identify any data quality issues

### Phase 4: Standardized Output

**Generate THREE artifacts** (all required):

#### 1. JSON: `{provider_slug}_catalog.json`
Raw course data in JSON format for programmatic access

#### 2. CSV: `{provider_slug}_catalog.csv`
Tabular format for analysis and comparison

#### 3. Google Sheet
Live, shareable spreadsheet (must provide URL to user)

**Required columns** (consistent across all providers):
- `provider` - Provider name (e.g., "HubSpot Academy")
- `title` - Course title
- `url` - Direct link to course page
- `description` - Course description/summary
- `duration` - Hours, modules, or time commitment
- `level` - Beginner/Intermediate/Advanced/All Levels
- `format` - On-Demand/Live/Blended/Self-Paced
- `price` - Free/Paid/$XX (as displayed)
- `category` - Topic/subject area
- `instructor` - If available
- `date_scraped` - ISO format date (YYYY-MM-DD)

**Optional columns** (include if available):
- `prerequisites`
- `learning_objectives`
- `certification_offered`
- `rating`
- `enrollment_count`
- `language`
- `last_updated`

**Implementation Code Snippet:**
```python
import json
from datetime import datetime
import pandas as pd
from utils.gsheets.save_gsheet import save_dataframe_to_new_sheet

def export_catalog_data(courses_data: list[dict], provider_slug: str) -> dict[str, str]:
    """Export to JSON, CSV, and Google Sheets."""
    # Convert to DataFrame
    df = pd.DataFrame(courses_data)

    # Column order
    column_order = [
        "provider", "title", "url", "description", "duration",
        "level", "format", "price", "category", "learning_path",
        "date_scraped"
    ]
    existing_cols = [col for col in column_order if col in df.columns]
    df = df[existing_cols]

    # 1. JSON
    json_filename = f"{provider_slug}_catalog.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(courses_data, f, indent=2, ensure_ascii=False)

    # 2. CSV
    csv_filename = f"{provider_slug}_catalog.csv"
    df.to_csv(csv_filename, index=False)

    # 3. Google Sheets
    # Function signature: save_dataframe_to_new_sheet(df, title, user_email='bianderson@linkedin.com')
    sheet_title = f"{provider_slug.title()} Course Catalog - {datetime.now().strftime('%Y-%m-%d')}"
    gsheet_url = save_dataframe_to_new_sheet(df, sheet_title)

    return {"json": json_filename, "csv": csv_filename, "gsheet_url": gsheet_url}
```

**Note**: `save_dataframe_to_new_sheet` parameters:
- `df` (DataFrame): The pandas DataFrame to export
- `title` (str): The Google Sheet title
- `user_email` (str, optional): Defaults to 'bianderson@linkedin.com'

**Generate Report: `{provider_slug}_report.md`**

Include:
```markdown
# {Provider Name} Catalog Scraping Report

**Date**: {date}
**URL**: {starting_url}
**Total Courses**: {count}

## Architecture
- Type: [Single Page / Paginated / Navigation-based / etc.]
- Data Source: [JSON / HTML / API / etc.]
- Obstacles: [None / Email gate / etc.]

## Extraction Method
[Description of strategy used]

## Data Quality
- Title: {X}% complete
- Description: {X}% complete
- Duration: {X}% complete
- Level: {X}% complete
- Price: {X}% complete

## Limitations
[Any access restrictions, missing data, or issues encountered]

## Recommendations
[Suggestions for improving data collection, e.g., "Obtain credentials for full access"]

## Sample Courses
[List 3-5 example courses with full metadata]
```

### Phase 5: Deliver Results

1. **Show Summary with ALL THREE artifact URLs**
   ```
   ✓ Scraped {X} courses from {Provider}

   ARTIFACTS GENERATED:
   ✓ JSON:   {provider_slug}_catalog.json
   ✓ CSV:    {provider_slug}_catalog.csv
   ✓ GSheet: {google_sheets_url}
   ✓ Report: {provider_slug}_report.md
   ```

2. **Display Google Sheets URL prominently** - This is the most valuable artifact for sharing/collaboration

3. **Preview Data** - Display first 3-5 courses with key fields

4. **Provide Analysis** - Quick insights:
   - Course count by level/category
   - Price distribution
   - Format breakdown
   - Content gaps or strengths
   - LinkedIn Learning licensing perspective

## Required Dependencies

All scrapers must include these imports:
```python
from __future__ import annotations

import json
import time
from datetime import datetime

import pandas as pd
import requests
from bs4 import BeautifulSoup
from utils.gsheets.save_gsheet import save_dataframe_to_new_sheet
```

**Custom Function Signature:**
```python
save_dataframe_to_new_sheet(
    df: pd.DataFrame,
    title: str,
    user_email: str = 'bianderson@linkedin.com'
) -> str  # Returns Google Sheets URL
```

Ensure `pandas` is in the project dependencies (already available in pyproject.toml).

## Best Practices

1. **Be Respectful**
   - Add delays between requests (1-2 seconds)
   - Respect robots.txt
   - Don't overwhelm servers
   - Use appropriate User-Agent

2. **Handle Failures Gracefully**
   - Log errors but continue scraping
   - Partial data is better than no data
   - Document what couldn't be accessed

3. **Validate URLs**
   - Test a sample course URL before full scrape
   - Ensure URL construction is correct
   - Handle edge cases (trailing slashes, query params)

4. **Be Transparent**
   - Document limitations clearly
   - Note what requires manual review
   - Explain data quality issues

## Common Patterns by Provider Type

### Corporate Training Platforms (HubSpot, Salesforce)
- Often have clean, structured course pages
- Usually server-rendered HTML
- May have free/gated content distinction
- Often well-organized by topic

### Tech/Developer Platforms (Anaconda, Pluralsight)
- May use JavaScript frameworks (React, etc.)
- Often have JSON data available
- Detailed technical metadata
- Strong categorization

### Professional Development (AMA, PMI)
- Mixed content types (courses, events, certifications)
- Often behind paywalls
- May require membership to view full catalog
- Less consistent structure

## Error Recovery

If scraping fails:
1. Document what was attempted
2. Provide what data IS available (even if incomplete)
3. Suggest alternative approaches
4. Offer to retry with modified strategy

## Output Location

**Local files** (current working directory):
- `{provider_slug}_catalog.json` - Raw JSON data
- `{provider_slug}_catalog.csv` - Tabular data
- `{provider_slug}_report.md` - Detailed scraping report
- `scrape_{provider_slug}.py` - Python script (for repeatability)

**Cloud artifacts**:
- Google Sheets URL (provided to user, accessible via browser)

## Success Criteria

**Minimum viable output** (MUST HAVE):
- ✓ JSON with at least: provider, title, url
- ✓ CSV with at least: provider, title, url
- ✓ Google Sheet with data (URL provided to user)
- ✓ Report documenting approach and limitations
- ✓ Working script for future updates

**Ideal output**:
- ✓ Complete metadata for all fields
- ✓ 100% of courses captured
- ✓ High data quality (>90% complete fields)
- ✓ Google Sheet formatted and shareable
- ✓ Reproducible process

**CRITICAL**: User MUST receive the Google Sheets URL - this is the primary deliverable for sharing and collaboration.
