from __future__ import annotations

import json
import re
import time
from datetime import datetime

import pandas as pd
import requests
from bs4 import BeautifulSoup
from utils.gsheets.save_gsheet import save_dataframe_to_new_sheet


# Browser headers to bypass CloudFront protection
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

BASE_URL = "https://academy.astronomer.io"


def fetch_page(url: str) -> BeautifulSoup:
    """Fetch and parse a page with proper headers."""
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    time.sleep(1)  # Be respectful
    return BeautifulSoup(response.text, "html.parser")


def get_learning_paths(soup: BeautifulSoup) -> list[dict]:
    """Extract learning path names and URLs from homepage."""
    paths = []

    # Find all links that start with /path/
    for link in soup.find_all("a", href=re.compile(r"^/path/[^/]+$")):
        path_url = link.get("href")
        # Get the path name from the link text or title
        path_name = link.get_text(strip=True)

        if path_url and path_url not in [p["url"] for p in paths]:
            paths.append({
                "name": path_name if path_name else path_url.split("/")[-1].replace("-", " ").title(),
                "url": f"{BASE_URL}{path_url}"
            })

    return paths


def get_courses_from_path(path_url: str) -> list[dict]:
    """Extract all courses from a learning path page."""
    soup = fetch_page(path_url)
    courses = []

    # Find course links within this path
    path_slug = path_url.split("/path/")[-1]
    for link in soup.find_all("a", href=re.compile(rf"^/path/{path_slug}/[^/]+$")):
        course_url = link.get("href")
        course_title = link.get_text(strip=True)

        if course_url and course_url not in [c["url"] for c in courses]:
            courses.append({
                "title": course_title,
                "url": f"{BASE_URL}{course_url}",
                "path": path_slug
            })

    return courses


def get_course_details(course_url: str) -> dict:
    """Scrape detailed metadata from a course page."""
    soup = fetch_page(course_url)
    details = {}

    # Extract title
    title_tag = soup.find("h1")
    if title_tag:
        details["title"] = title_tag.get_text(strip=True)

    # Extract description
    meta_desc = soup.find("meta", {"name": "description"})
    if meta_desc:
        details["description"] = meta_desc.get("content", "")

    # Look for duration/time estimate
    for elem in soup.find_all(["span", "div", "p"]):
        text = elem.get_text(strip=True)
        if re.search(r"\d+\s*(min|hour|hr)", text, re.IGNORECASE):
            details["duration"] = text
            break

    # Look for level
    for elem in soup.find_all(["span", "div", "p"]):
        text = elem.get_text(strip=True)
        if any(level in text for level in ["Beginner", "Intermediate", "Advanced"]):
            details["level"] = text
            break

    # Price (likely free for Astronomer Academy)
    details["price"] = "Free"

    return details


def export_catalog_data(courses_data: list[dict], provider_slug: str) -> dict[str, str]:
    """
    Export course catalog to JSON, CSV, and Google Sheets.

    Args:
        courses_data: List of course dictionaries
        provider_slug: Provider name slug (e.g., "astronomer")

    Returns:
        Dictionary with paths/URLs for all three artifacts
    """
    if not courses_data:
        print("No courses to export")
        return {}

    # Prepare data for DataFrame
    df_data = []
    for course in courses_data:
        df_data.append({
            "provider": "Astronomer Academy",
            "title": course.get("title", ""),
            "url": course.get("url", ""),
            "description": course.get("description", ""),
            "duration": course.get("duration", ""),
            "level": course.get("level", ""),
            "format": "On-Demand",
            "price": course.get("price", "Free"),
            "category": "Apache Airflow / Data Engineering",
            "learning_path": course.get("path", ""),
            "date_scraped": datetime.now().strftime("%Y-%m-%d"),
        })

    # Convert to DataFrame
    df = pd.DataFrame(df_data)

    # Column order
    column_order = [
        "provider", "title", "url", "description", "duration",
        "level", "format", "price", "category", "learning_path",
        "date_scraped"
    ]
    existing_cols = [col for col in column_order if col in df.columns]
    df = df[existing_cols]

    # 1. Export to JSON
    json_filename = f"{provider_slug}_catalog.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(df_data, f, indent=2, ensure_ascii=False)
    print(f"âœ“ Exported to JSON: {json_filename}")

    # 2. Export to CSV
    csv_filename = f"{provider_slug}_catalog.csv"
    df.to_csv(csv_filename, index=False)
    print(f"âœ“ Exported to CSV: {csv_filename}")

    # 3. Export to Google Sheets
    sheet_title = f"{provider_slug.title()} Course Catalog - {datetime.now().strftime('%Y-%m-%d')}"
    print(f"âœ“ Creating Google Sheet: {sheet_title}")
    gsheet_url = save_dataframe_to_new_sheet(
        df=df,
        title=sheet_title
    )
    print(f"âœ“ Google Sheet created: {gsheet_url}")

    return {
        "json": json_filename,
        "csv": csv_filename,
        "gsheet_url": gsheet_url,
    }


if __name__ == "__main__":
    print("Scraping Astronomer Academy...")
    print("=" * 80)

    # Step 1: Get homepage and find learning paths
    print("\n[1/3] Discovering learning paths...")
    homepage = fetch_page(BASE_URL)
    paths = get_learning_paths(homepage)
    print(f"Found {len(paths)} learning paths")

    # Step 2: Get courses from each path
    print("\n[2/3] Extracting courses from each path...")
    all_courses = []
    for i, path in enumerate(paths, 1):
        print(f"  [{i}/{len(paths)}] {path['name']}...")
        try:
            courses = get_courses_from_path(path["url"])
            all_courses.extend(courses)
            print(f"      â†’ Found {len(courses)} courses")
        except Exception as e:
            print(f"      â†’ Error: {e}")

    print(f"\nTotal courses found: {len(all_courses)}")

    # Step 3: Get detailed metadata for each course
    print("\n[3/3] Fetching course details...")
    for i, course in enumerate(all_courses, 1):
        print(f"  [{i}/{len(all_courses)}] {course['title'][:60]}...")
        try:
            details = get_course_details(course["url"])
            course.update(details)
        except Exception as e:
            print(f"      â†’ Error: {e}")

    # Export results
    print("\n" + "=" * 80)
    print("EXPORTING CATALOG")
    print("=" * 80 + "\n")

    provider_slug = "astronomer"
    artifacts = export_catalog_data(all_courses, provider_slug)

    # Display all artifacts
    print("\n" + "=" * 80)
    print("âœ“ SCRAPING COMPLETE - ALL ARTIFACTS GENERATED")
    print("=" * 80)
    print(f"\nðŸ“Š Total Courses: {len(all_courses)}")
    print(f"\nARTIFACTS:")
    print(f"  ðŸ“„ JSON:   {artifacts['json']}")
    print(f"  ðŸ“Š CSV:    {artifacts['csv']}")
    print(f"  ðŸ”— GSheet: {artifacts['gsheet_url']}")
    print()

    # Show preview
    print("=" * 80)
    print("PREVIEW - First 3 Courses")
    print("=" * 80)
    for i, course in enumerate(all_courses[:3], 1):
        print(f"\n{i}. {course['title']}")
        print(f"   Path: {course.get('path', 'N/A')}")
        print(f"   URL: {course['url']}")
        if course.get("description"):
            desc = course['description']
            if len(desc) > 100:
                desc = desc[:100] + "..."
            print(f"   Description: {desc}")
    print()
