from __future__ import annotations

import json
import re
import time
from datetime import datetime

import pandas as pd
import requests
from bs4 import BeautifulSoup

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def scrape_mailchimp_improved() -> list[dict]:
    """Improved Mailchimp scraper targeting actual course cards."""
    url = "https://www.mailchimpacademy.com/student/catalog"
    print(f"Scraping Mailchimp Academy...")

    response = requests.get(url, headers=HEADERS, timeout=15)
    soup = BeautifulSoup(response.content, 'html.parser')

    courses = []

    # Look for actual course cards - they should have both title and description
    course_cards = soup.find_all('div', class_=re.compile(r'course|offering'))

    for card in course_cards:
        # Must have a link
        link = card.find('a', href=True)
        if not link:
            continue

        href = link['href']

        # Skip navigation items
        if any(skip in href for skip in ['all_sessions', 'activity', 'linkedin.com']):
            continue

        # Must have actual content
        title = link.get_text(strip=True)
        if len(title) < 5:
            continue

        # Look for description
        desc_elem = card.find('p') or card.find('div', class_=re.compile(r'desc|summary'))
        description = desc_elem.get_text(strip=True) if desc_elem else ""

        # Normalize URL
        if not href.startswith('http'):
            href = f"https://www.mailchimpacademy.com{href}"

        courses.append({
            'provider': 'Mailchimp Academy',
            'title': title,
            'url': href,
            'description': description,
            'duration': '',
            'level': '',
            'format': 'On-Demand',
            'price': 'Free',
            'category': 'Email Marketing',
            'date_scraped': datetime.now().strftime('%Y-%m-%d')
        })

    print(f"  Found {len(courses)} courses")
    return courses

def scrape_american_staffing_improved() -> list[dict]:
    """Scrape American Staffing - try multiple pages."""
    base_url = "https://learn.americanstaffing.net"

    print(f"Scraping American Staffing...")

    # Try different potential catalog URLs
    urls_to_try = [
        f"{base_url}/",
        f"{base_url}/onboardingCourses",
        f"{base_url}/catalog",
        f"{base_url}/courses",
    ]

    courses = []

    for url in urls_to_try:
        try:
            response = requests.get(url, headers=HEADERS, timeout=15)
            if response.status_code != 200:
                continue

            soup = BeautifulSoup(response.content, 'html.parser')

            # Look for course listings
            course_elements = soup.find_all(['div', 'article', 'li'], class_=re.compile(r'course|class|training'))

            for elem in course_elements:
                title_elem = elem.find(['h1', 'h2', 'h3', 'h4', 'a'])
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                if len(title) < 5:
                    continue

                link = elem.find('a', href=True)
                href = link['href'] if link else ""
                if href and not href.startswith('http'):
                    href = f"{base_url}{href}"

                desc = elem.find('p')
                description = desc.get_text(strip=True) if desc else ""

                courses.append({
                    'provider': 'American Staffing Association',
                    'title': title,
                    'url': href,
                    'description': description,
                    'duration': '',
                    'level': '',
                    'format': 'On-Demand',
                    'price': '',
                    'category': 'Staffing',
                    'date_scraped': datetime.now().strftime('%Y-%m-%d')
                })

            if courses:
                break

        except Exception as e:
            continue

    print(f"  Found {len(courses)} courses")
    return courses

def scrape_jetbrains_improved() -> list[dict]:
    """Scrape JetBrains Academy - look for learning tracks."""
    url = "https://www.jetbrains.com/academy/"

    print(f"Scraping JetBrains Academy...")

    response = requests.get(url, headers=HEADERS, timeout=15)
    soup = BeautifulSoup(response.content, 'html.parser')

    courses = []

    # JetBrains might have tracks/courses in specific sections
    # Look for headings that might indicate courses
    headers = soup.find_all(['h2', 'h3'], string=re.compile(r'(learn|track|course|path)', re.I))

    for header in headers:
        # Find associated content
        parent = header.find_parent(['div', 'section'])
        if not parent:
            continue

        links = parent.find_all('a', href=True)
        for link in links:
            text = link.get_text(strip=True)
            if len(text) < 5:
                continue

            href = link['href']
            if not href.startswith('http'):
                href = f"https://www.jetbrains.com{href}"

            courses.append({
                'provider': 'JetBrains Academy',
                'title': text,
                'url': href,
                'description': '',
                'duration': '',
                'level': '',
                'format': 'On-Demand',
                'price': '',
                'category': 'Software Development',
                'date_scraped': datetime.now().strftime('%Y-%m-%d')
            })

    # Also check structured data
    ld_json = soup.find_all('script', type='application/ld+json')
    for script in ld_json:
        try:
            data = json.loads(script.string)
            if isinstance(data, dict) and data.get('@type') == 'Course':
                courses.append({
                    'provider': 'JetBrains Academy',
                    'title': data.get('name', ''),
                    'url': data.get('url', ''),
                    'description': data.get('description', ''),
                    'duration': '',
                    'level': '',
                    'format': 'On-Demand',
                    'price': '',
                    'category': 'Software Development',
                    'date_scraped': datetime.now().strftime('%Y-%m-%d')
                })
        except:
            continue

    print(f"  Found {len(courses)} courses")
    return courses

def scrape_zendesk_improved() -> list[dict]:
    """Scrape Zendesk - the page likely loads content dynamically."""
    url = "https://training.zendesk.com/page/zendesk-course-catalog"

    print(f"Scraping Zendesk Training...")

    response = requests.get(url, headers=HEADERS, timeout=15)
    soup = BeautifulSoup(response.content, 'html.parser')

    courses = []

    # Check for JSON script with course data
    scripts = soup.find_all('script', type='application/json')
    for script in scripts:
        try:
            data = json.loads(script.string)

            def extract_courses(obj):
                if isinstance(obj, dict):
                    # Check if this looks like course data
                    if 'title' in obj or 'name' in obj:
                        if 'course' in str(obj).lower() or 'training' in str(obj).lower():
                            courses.append(obj)
                    for value in obj.values():
                        if isinstance(value, (dict, list)):
                            extract_courses(value)
                elif isinstance(obj, list):
                    for item in obj:
                        extract_courses(item)

            extract_courses(data)
        except:
            continue

    # Convert to standard format
    standardized = []
    for c in courses:
        standardized.append({
            'provider': 'Zendesk Training',
            'title': c.get('title') or c.get('name', ''),
            'url': c.get('url') or c.get('link', ''),
            'description': c.get('description', ''),
            'duration': c.get('duration', ''),
            'level': c.get('level', ''),
            'format': 'On-Demand',
            'price': c.get('price', 'Free'),
            'category': c.get('category', 'Customer Support'),
            'date_scraped': datetime.now().strftime('%Y-%m-%d')
        })

    print(f"  Found {len(standardized)} courses from JSON")

    # Also try HTML parsing
    if not standardized:
        # Look for course cards
        cards = soup.find_all(['div', 'article'], class_=re.compile(r'course|offering|card'))
        for card in cards[:100]:
            title_elem = card.find(['h1', 'h2', 'h3', 'h4'])
            if not title_elem:
                continue

            title = title_elem.get_text(strip=True)
            link = card.find('a', href=True)
            href = link['href'] if link else ""

            if title and len(title) > 5:
                standardized.append({
                    'provider': 'Zendesk Training',
                    'title': title,
                    'url': href if href.startswith('http') else f"https://training.zendesk.com{href}",
                    'description': '',
                    'duration': '',
                    'level': '',
                    'format': 'On-Demand',
                    'price': 'Free',
                    'category': 'Customer Support',
                    'date_scraped': datetime.now().strftime('%Y-%m-%d')
                })

        print(f"  Found {len(standardized)} courses from HTML")

    return standardized

def scrape_astronomer_improved() -> list[dict]:
    """Improved Astronomer scraper - look for actual course pages."""
    url = "https://academy.astronomer.io/"

    print(f"Scraping Astronomer Academy...")

    response = requests.get(url, headers=HEADERS, timeout=15)
    soup = BeautifulSoup(response.content, 'html.parser')

    courses = []

    # Look for "View Course" or similar links
    all_links = soup.find_all('a', href=True)

    for link in all_links:
        href = link['href']
        text = link.get_text(strip=True)

        # Look for course/path links
        if 'course' in href.lower() or 'path' in href.lower():
            if not href.startswith('http'):
                href = f"https://academy.astronomer.io{href}"

            if len(text) > 5:
                courses.append({
                    'provider': 'Astronomer Academy',
                    'title': text,
                    'url': href,
                    'description': '',
                    'duration': '',
                    'level': '',
                    'format': 'On-Demand',
                    'price': 'Free',
                    'category': 'Apache Airflow',
                    'date_scraped': datetime.now().strftime('%Y-%m-%d')
                })

    # Also look for cards with course information
    cards = soup.find_all('div', class_=re.compile(r'card|course'))
    for card in cards:
        title_elem = card.find(['h1', 'h2', 'h3'])
        if not title_elem:
            continue

        title = title_elem.get_text(strip=True)
        if len(title) < 10:
            continue

        link = card.find('a', href=True)
        href = link['href'] if link else ""
        if href and not href.startswith('http'):
            href = f"https://academy.astronomer.io{href}"

        desc = card.find('p')
        description = desc.get_text(strip=True) if desc else ""

        courses.append({
            'provider': 'Astronomer Academy',
            'title': title,
            'url': href,
            'description': description,
            'duration': '',
            'level': '',
            'format': 'On-Demand',
            'price': 'Free',
            'category': 'Apache Airflow',
            'date_scraped': datetime.now().strftime('%Y-%m-%d')
        })

    print(f"  Found {len(courses)} courses")
    return courses

def export_data(courses: list[dict], slug: str):
    """Export to JSON, CSV, and report."""
    if not courses:
        print(f"  No courses to export for {slug}")
        return

    # Remove duplicates
    seen = set()
    unique = []
    for c in courses:
        key = c['title'].lower()
        if key not in seen and len(key) > 5:
            seen.add(key)
            unique.append(c)

    # Export JSON
    json_file = f"{slug}_catalog.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(unique, f, indent=2, ensure_ascii=False)

    # Export CSV
    csv_file = f"{slug}_catalog.csv"
    df = pd.DataFrame(unique)
    df.to_csv(csv_file, index=False)

    # Generate report
    report_file = f"{slug}_report.md"
    report = f"""# {unique[0]['provider']} Catalog Scraping Report

**Date**: {datetime.now().strftime('%Y-%m-%d')}
**Total Courses**: {len(unique)}

## Sample Courses

"""

    for i, course in enumerate(unique[:10], 1):
        report += f"### {i}. {course['title']}\n"
        report += f"- **URL**: {course['url']}\n"
        if course['description']:
            report += f"- **Description**: {course['description'][:200]}\n"
        report += "\n"

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"  âœ“ Exported {len(unique)} courses")
    print(f"    - {json_file}")
    print(f"    - {csv_file}")
    print(f"    - {report_file}")

def main():
    """Re-scrape problematic providers."""

    scrapers = [
        ('mailchimp', scrape_mailchimp_improved),
        ('american_staffing', scrape_american_staffing_improved),
        ('jetbrains', scrape_jetbrains_improved),
        ('zendesk', scrape_zendesk_improved),
        ('astronomer', scrape_astronomer_improved),
    ]

    for slug, scraper_func in scrapers:
        print(f"\n{'='*60}")
        courses = scraper_func()
        if courses:
            export_data(courses, slug)
        time.sleep(2)

    print(f"\n{'='*60}")
    print("TARGETED SCRAPING COMPLETE")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
