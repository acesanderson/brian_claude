from __future__ import annotations

import json
import re
import time
from datetime import datetime

import pandas as pd
import requests
from bs4 import BeautifulSoup

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

PROVIDERS = [
    ("ama", "AMA", "https://www.ama.org/topics/"),
    ("american_staffing", "American Staffing Association", "https://learn.americanstaffing.net/"),
    ("arista", "Arista Networks", "https://www.training.arista.com/"),
    ("astronomer", "Astronomer Academy", "https://academy.astronomer.io/"),
    ("canonical", "Canonical Academy", "https://canonical.com/academy/exam-content?exam=Using%20Ubuntu%20Server"),
    ("mailchimp", "Mailchimp Academy", "https://www.mailchimpacademy.com/student/catalog"),
    ("jetbrains", "JetBrains Academy", "https://www.jetbrains.com/academy/"),
    ("sales_management", "Sales Management Association", "https://salesmanagement.org/courses/"),
    ("zendesk", "Zendesk Training", "https://training.zendesk.com/page/zendesk-course-catalog#"),
]

def scrape_provider(slug: str, name: str, url: str) -> list[dict]:
    """Generic scraper that adapts to different site structures."""
    print(f"\n{'='*60}")
    print(f"Scraping: {name}")
    print(f"URL: {url}")
    print(f"{'='*60}")

    try:
        response = requests.get(url, headers=HEADERS, timeout=20)
        if response.status_code != 200:
            print(f"Error: HTTP {response.status_code}")
            return []

        soup = BeautifulSoup(response.content, 'html.parser')
        courses = []

        # Strategy 1: Look for JSON data
        json_scripts = soup.find_all('script', type='application/json')
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                extracted = extract_courses_from_json(data)
                if extracted:
                    courses.extend(extracted)
                    print(f"  Found {len(extracted)} courses in JSON")
            except:
                pass

        # Strategy 2: Parse HTML course cards/lists
        if not courses:
            courses = extract_courses_from_html(soup, url)
            print(f"  Found {len(courses)} courses in HTML")

        # Standardize format
        standardized = []
        for course in courses:
            standardized.append({
                'provider': name,
                'title': clean_text(course.get('title') or course.get('name', '')),
                'url': normalize_url(course.get('url') or course.get('link', ''), url),
                'description': clean_text(course.get('description', '')),
                'duration': clean_text(course.get('duration', '')),
                'level': clean_text(course.get('level', '')),
                'format': clean_text(course.get('format', 'On-Demand')),
                'price': clean_text(course.get('price', '')),
                'category': clean_text(course.get('category', '')),
                'date_scraped': datetime.now().strftime('%Y-%m-%d')
            })

        # Remove duplicates based on title
        seen = set()
        unique = []
        for course in standardized:
            title_lower = course['title'].lower()
            if title_lower and title_lower not in seen and len(title_lower) > 5:
                seen.add(title_lower)
                unique.append(course)

        print(f"  ✓ Total unique courses: {len(unique)}")
        return unique

    except Exception as e:
        print(f"  Error scraping {name}: {str(e)}")
        return []

def extract_courses_from_json(data: dict | list, courses: list = None) -> list[dict]:
    """Recursively search JSON for course-like objects."""
    if courses is None:
        courses = []

    if isinstance(data, dict):
        # Check if this looks like a course
        if ('title' in data or 'name' in data) and len(data) > 2:
            courses.append(data)
        # Recurse into values
        for value in data.values():
            if isinstance(value, (dict, list)):
                extract_courses_from_json(value, courses)
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, (dict, list)):
                extract_courses_from_json(item, courses)

    return courses

def extract_courses_from_html(soup: BeautifulSoup, base_url: str) -> list[dict]:
    """Extract courses from HTML structure."""
    courses = []

    # Try multiple selectors
    selectors = [
        ('div', re.compile(r'course|class|offering')),
        ('article', None),
        ('li', re.compile(r'course|item')),
        ('div', re.compile(r'card')),
    ]

    for tag, class_pattern in selectors:
        if class_pattern:
            elements = soup.find_all(tag, class_=class_pattern)
        else:
            elements = soup.find_all(tag)

        if len(elements) > 2:  # If we found multiple elements
            for elem in elements[:200]:  # Limit to 200
                course = parse_course_element(elem)
                if course and course.get('title'):
                    courses.append(course)

            if courses:
                break  # Found courses, don't try other selectors

    return courses

def parse_course_element(elem) -> dict:
    """Extract course data from an HTML element."""
    # Find title
    title_elem = elem.find(['h1', 'h2', 'h3', 'h4', 'h5', 'a'])
    title = title_elem.get_text(strip=True) if title_elem else ""

    # Find URL
    link_elem = elem.find('a', href=True)
    url = link_elem['href'] if link_elem else ""

    # Find description
    desc_elem = elem.find('p') or elem.find('div', class_=re.compile(r'desc|summary'))
    description = desc_elem.get_text(strip=True) if desc_elem else ""

    # Find duration
    duration_elem = elem.find(string=re.compile(r'\d+\s*(hour|min|week|day)', re.I))
    duration = duration_elem.strip() if duration_elem else ""

    # Find level
    level_elem = elem.find(string=re.compile(r'beginner|intermediate|advanced', re.I))
    level = level_elem.strip() if level_elem else ""

    # Find price
    price_elem = elem.find(string=re.compile(r'free|paid|\$\d+', re.I))
    price = price_elem.strip() if price_elem else ""

    return {
        'title': title,
        'url': url,
        'description': description,
        'duration': duration,
        'level': level,
        'price': price
    }

def clean_text(text: str) -> str:
    """Clean and normalize text."""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def normalize_url(url: str, base_url: str) -> str:
    """Normalize relative URLs to absolute."""
    if not url:
        return ""
    if url.startswith('http'):
        return url
    if url.startswith('/'):
        from urllib.parse import urlparse
        parsed = urlparse(base_url)
        return f"{parsed.scheme}://{parsed.netloc}{url}"
    return url

def export_data(courses: list[dict], slug: str) -> dict[str, str]:
    """Export to JSON and CSV."""
    if not courses:
        print(f"  No courses to export for {slug}")
        return {'json': '', 'csv': '', 'count': 0}

    df = pd.DataFrame(courses)

    json_file = f"{slug}_catalog.json"
    csv_file = f"{slug}_catalog.csv"

    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(courses, f, indent=2, ensure_ascii=False)

    df.to_csv(csv_file, index=False)

    return {'json': json_file, 'csv': csv_file, 'count': len(courses)}

def generate_report(slug: str, name: str, url: str, courses: list[dict]) -> str:
    """Generate markdown report."""
    report_file = f"{slug}_report.md"

    # Calculate completeness
    total = len(courses)
    if total == 0:
        completeness = {k: 0 for k in ['title', 'description', 'duration', 'level', 'price']}
    else:
        completeness = {
            'title': sum(1 for c in courses if c.get('title')) / total * 100,
            'description': sum(1 for c in courses if c.get('description')) / total * 100,
            'duration': sum(1 for c in courses if c.get('duration')) / total * 100,
            'level': sum(1 for c in courses if c.get('level')) / total * 100,
            'price': sum(1 for c in courses if c.get('price')) / total * 100,
        }

    report = f"""# {name} Catalog Scraping Report

**Date**: {datetime.now().strftime('%Y-%m-%d')}
**URL**: {url}
**Total Courses**: {total}

## Architecture
- Type: Single Page / HTML Parsing
- Data Source: HTML + JSON (where available)
- Obstacles: None detected

## Extraction Method
Used adaptive scraping strategy:
1. Checked for embedded JSON data
2. Parsed HTML course cards/listings
3. Extracted metadata from structured elements

## Data Quality
- Title: {completeness['title']:.1f}% complete
- Description: {completeness['description']:.1f}% complete
- Duration: {completeness['duration']:.1f}% complete
- Level: {completeness['level']:.1f}% complete
- Price: {completeness['price']:.1f}% complete

## Sample Courses
"""

    for i, course in enumerate(courses[:5], 1):
        report += f"\n### {i}. {course['title']}\n"
        report += f"- **URL**: {course['url']}\n"
        if course['description']:
            report += f"- **Description**: {course['description'][:200]}...\n"
        if course['duration']:
            report += f"- **Duration**: {course['duration']}\n"
        if course['level']:
            report += f"- **Level**: {course['level']}\n"
        if course['price']:
            report += f"- **Price**: {course['price']}\n"

    report += f"""

## Recommendations
- Review course URLs for accuracy
- Consider deeper crawl for additional metadata
- Verify price information if purchasing decisions needed
"""

    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    return report_file

def main():
    """Scrape all providers."""
    all_results = []

    for slug, name, url in PROVIDERS:
        courses = scrape_provider(slug, name, url)
        time.sleep(2)  # Be respectful

        if courses:
            result = export_data(courses, slug)
            report_file = generate_report(slug, name, url, courses)

            all_results.append({
                'provider': name,
                'slug': slug,
                'count': result['count'],
                'json_file': result['json'],
                'csv_file': result['csv'],
                'report_file': report_file
            })

            print(f"  ✓ JSON: {result['json']}")
            print(f"  ✓ CSV: {result['csv']}")
            print(f"  ✓ Report: {report_file}")

    # Summary
    print(f"\n{'='*60}")
    print("SCRAPING COMPLETE")
    print(f"{'='*60}")
    for result in all_results:
        print(f"\n{result['provider']}: {result['count']} courses")
        print(f"  - {result['json_file']}")
        print(f"  - {result['csv_file']}")
        print(f"  - {result['report_file']}")

    # Save summary
    with open('scraping_summary.json', 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2)

if __name__ == "__main__":
    main()
