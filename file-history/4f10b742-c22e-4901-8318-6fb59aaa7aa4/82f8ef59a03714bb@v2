from __future__ import annotations

import requests
from bs4 import BeautifulSoup
import json
import time

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

PROVIDERS = [
    ("AMA", "https://www.ama.org/topics/"),
    ("American Staffing", "https://learn.americanstaffing.net/"),
    ("Arista", "https://www.training.arista.com/"),
    ("Astronomer", "https://academy.astronomer.io/"),
    ("Canonical", "https://canonical.com/academy/exam-content?exam=Using%20Ubuntu%20Server"),
    ("Mailchimp", "https://www.mailchimpacademy.com/student/catalog"),
    ("JetBrains", "https://www.jetbrains.com/academy/"),
    ("Sales Management", "https://salesmanagement.org/courses/"),
    ("Zendesk", "https://training.zendesk.com/page/zendesk-course-catalog#"),
]

def analyze_provider(name: str, url: str) -> dict:
    """Analyze a provider's page structure."""
    print(f"\n{'='*60}")
    print(f"Analyzing: {name}")
    print(f"URL: {url}")
    print(f"{'='*60}")

    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        print(f"Status: {response.status_code}")

        if response.status_code != 200:
            return {
                "provider": name,
                "url": url,
                "status": response.status_code,
                "error": f"HTTP {response.status_code}",
                "architecture": "Unknown - Access Denied"
            }

        soup = BeautifulSoup(response.content, 'html.parser')

        # Check for JSON data sources
        json_scripts = soup.find_all('script', type='application/json')
        ld_json = soup.find_all('script', type='application/ld+json')
        hidden_inputs = soup.find_all('input', type='hidden')

        # Check for common course card patterns
        course_cards = []
        common_selectors = [
            'div[class*="course"]',
            'div[class*="card"]',
            'article',
            'li[class*="course"]',
            'div[class*="catalog"]',
            'div[class*="item"]'
        ]

        for selector in common_selectors:
            elements = soup.select(selector)
            if len(elements) > 3:
                course_cards.append((selector, len(elements)))

        # Check for pagination
        pagination = soup.find_all(['nav', 'div'], class_=lambda x: x and any(p in str(x).lower() for p in ['pagination', 'pager', 'next', 'prev']))

        # Check title
        title = soup.find('title')
        h1 = soup.find('h1')

        result = {
            "provider": name,
            "url": url,
            "status": response.status_code,
            "title": title.text.strip() if title else "N/A",
            "h1": h1.text.strip() if h1 else "N/A",
            "json_scripts": len(json_scripts),
            "ld_json": len(ld_json),
            "hidden_inputs": len(hidden_inputs),
            "potential_course_cards": course_cards[:3] if course_cards else [],
            "has_pagination": len(pagination) > 0,
            "content_length": len(response.content)
        }

        print(f"  Title: {result['title'][:80]}")
        print(f"  H1: {result['h1'][:80]}")
        print(f"  JSON scripts: {result['json_scripts']}")
        print(f"  LD+JSON: {result['ld_json']}")
        print(f"  Hidden inputs: {result['hidden_inputs']}")
        print(f"  Potential course elements: {len(course_cards)}")
        if course_cards:
            for selector, count in course_cards[:3]:
                print(f"    - {selector}: {count} elements")
        print(f"  Has pagination: {result['has_pagination']}")

        return result

    except Exception as e:
        print(f"Error: {str(e)}")
        return {
            "provider": name,
            "url": url,
            "error": str(e),
            "architecture": "Unknown - Error"
        }

def main():
    """Analyze all providers."""
    results = []

    for name, url in PROVIDERS:
        result = analyze_provider(name, url)
        results.append(result)
        time.sleep(2)  # Be respectful

    # Save results
    with open('provider_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)

    print(f"\n{'='*60}")
    print(f"Analysis complete. Results saved to provider_analysis.json")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
