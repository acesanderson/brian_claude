# Batch-Dispatch Observability Improvements

**Date:** 2026-02-09
**Context:** Identified during training catalog scraping project where 38 workers ran for 2+ hours with all tasks timing out, discovered only at completion.

## Problem Statement

The current batch-dispatch system provides zero visibility during execution, leading to:
- **Wasted time:** Hours spent running broken workers before discovering failures
- **No early detection:** Configuration errors only surface after full batch completion
- **Poor debugging:** No logs or diagnostics when tasks fail
- **All-or-nothing results:** Can't see incremental progress or partial successes

## Proposed Improvements

### 1. Pre-flight Validation (HIGHEST PRIORITY)

**Problem:** We don't know if the skill works until after launching 38+ workers.

**Solution:** Test on first item before launching full batch.

**Implementation:**
```python
async def validate_skill(skill_name, first_item, template, timeout):
    """Run skill on first item as validation before launching full batch."""
    print(f"\nüß™ PRE-FLIGHT: Testing {skill_name} on first item...")

    # Run single worker
    result = await run_worker(
        sem=asyncio.Semaphore(1),
        run_id="preflight",
        index=0,
        item=first_item,
        skill_name=skill_name,
        user_template=template,
        timeout=timeout
    )

    # Check result
    if result["status"] == "success":
        print(f"‚úÖ PRE-FLIGHT PASSED: Skill working correctly")
        print(f"   Output: {result.get('data', {}).keys()}")
        return True
    elif result["status"] == "timeout":
        print(f"‚ùå PRE-FLIGHT FAILED: Task timed out after {timeout}s")
        print(f"   Consider increasing --timeout or debugging skill")
        return False
    else:
        print(f"‚ùå PRE-FLIGHT FAILED: {result.get('error', 'Unknown error')}")
        print(f"   Task dir: {result.get('task_dir')}")
        return False

# In main():
if not args.skip_preflight:
    success = await validate_skill(args.skill, inputs[0], args.template, args.timeout)
    if not success:
        print("\n‚ö†Ô∏è  Pre-flight validation failed. Abort batch? (y/n)")
        # ... handle user input or auto-abort
```

**Benefits:**
- Catch errors in 20 minutes instead of 12+ hours
- Verify skill invocation works correctly
- Test timeout adequacy on one item

**CLI Flag:**
```bash
--skip-preflight  # Skip validation (for trusted workflows)
```

---

### 2. Real-time Progress Monitoring (HIGH PRIORITY)

**Problem:** No visibility into worker health during execution.

**Solution:** Periodic health checks every 2-5 minutes showing file counts and completion status.

**Implementation:**
```python
async def monitor_progress(run_dir, total_tasks, interval=120):
    """Background task that monitors and reports progress."""
    while True:
        await asyncio.sleep(interval)

        # Scan task directories
        completed = 0
        active = 0
        has_files = 0

        for i in range(total_tasks):
            task_dir = os.path.join(run_dir, f"task_{i}")
            if not os.path.exists(task_dir):
                continue

            # Check for result.json (completed)
            result_file = os.path.join(task_dir, "result.json")
            if os.path.exists(result_file):
                completed += 1
                continue

            # Check for any files (worker active)
            files = os.listdir(task_dir)
            if len(files) > 0:
                has_files += 1

            # Check if worker process exists
            # (could track PIDs in shared state)

        # Report
        print(f"\nüìä PROGRESS ({datetime.now().strftime('%H:%M:%S')})")
        print(f"   Completed: {completed}/{total_tasks}")
        print(f"   Has output: {has_files} tasks")
        print(f"   Success rate: {completed/total_tasks*100:.0f}%")

        # Early warning
        if completed + has_files == 0 and elapsed_time > 300:  # 5 min
            print(f"   ‚ö†Ô∏è  WARNING: No output detected after 5 minutes")

# In main():
monitor_task = asyncio.create_task(
    monitor_progress(run_dir, len(inputs), interval=120)
)
```

**Output Example:**
```
üìä PROGRESS (14:37:22)
   Completed: 3/38
   Has output: 5 tasks
   Success rate: 8%

üìä PROGRESS (14:39:22)
   Completed: 7/38
   Has output: 5 tasks
   Success rate: 18%
```

**Benefits:**
- Know within 5-10 minutes if workers are producing output
- Spot problems early instead of waiting hours
- Understand completion velocity

---

### 3. Worker Logging (MEDIUM PRIORITY)

**Problem:** When tasks fail, we have no debugging information.

**Solution:** Capture stdout/stderr from each worker to log files.

**Implementation:**
```python
# In run_worker():
stdout_log = os.path.join(task_dir, "worker.stdout")
stderr_log = os.path.join(task_dir, "worker.stderr")

with open(stdout_log, "wb") as out_file, open(stderr_log, "wb") as err_file:
    process = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=out_file,  # Stream to file instead of PIPE
        stderr=err_file,
        cwd=task_dir,
    )

    await asyncio.wait_for(process.wait(), timeout=timeout)

# On failure, read and report logs
if process.returncode != 0:
    with open(stderr_log, "r") as f:
        error_output = f.read()[-500:]  # Last 500 chars
    return {
        "status": "error",
        "error": error_output,
        "logs": {"stdout": stdout_log, "stderr": stderr_log}
    }
```

**Benefits:**
- Can diagnose failures without re-running
- See actual error messages from workers
- Debug skill invocation issues

**Artifacts:**
- `task_N/worker.stdout` - Worker output
- `task_N/worker.stderr` - Worker errors
- Included in final summary.json for failed tasks

---

### 4. Fail-Fast Mode (MEDIUM PRIORITY)

**Problem:** Waste hours running remaining tasks when something is fundamentally broken.

**Solution:** Option to abort batch if first N tasks all fail/timeout.

**Implementation:**
```python
# Add argument
parser.add_argument(
    "--fail-fast",
    type=int,
    default=0,
    help="Abort if first N tasks fail/timeout (0=disabled)"
)

# In main(), after launching first batch of tasks:
if args.fail_fast > 0:
    # Wait for first N tasks to complete
    first_n_results = results[:args.fail_fast]
    failures = sum(1 for r in first_n_results if r["status"] != "success")

    if failures == args.fail_fast:
        print(f"\n‚ùå FAIL-FAST: All of first {args.fail_fast} tasks failed")
        print(f"   Aborting remaining {len(inputs) - args.fail_fast} tasks")
        print(f"   Review logs in: {run_dir}")
        # Cancel remaining tasks
        for task in remaining_tasks:
            task.cancel()
        sys.exit(1)
```

**CLI Usage:**
```bash
# Abort if first 3 tasks fail
batch_runner.py ... --fail-fast=3

# Abort if first 5 tasks timeout
batch_runner.py ... --fail-fast=5
```

**Benefits:**
- Don't waste hours when configuration is wrong
- Get immediate feedback on systemic issues
- Preserve resources for other work

---

### 5. Incremental Results (LOW PRIORITY)

**Problem:** Can't see any results until entire batch completes.

**Solution:** Copy output files to results directory as tasks complete.

**Implementation:**
```python
def save_incremental_result(result, output_dir):
    """Save result immediately upon task completion."""
    if result["status"] != "success":
        return

    task_dir = result["task_dir"]

    # Copy artifacts to output directory
    for filename in os.listdir(task_dir):
        if filename.endswith((".json", ".xlsx", ".md", ".csv")):
            if filename != "result.json":
                src = os.path.join(task_dir, filename)
                dst = os.path.join(output_dir, filename)
                shutil.copy2(src, dst)
                print(f"‚úì Saved: {filename}")

# In main(), as tasks complete:
for coro in asyncio.as_completed(tasks):
    result = await coro
    results.append(result)

    # Save immediately
    if result["status"] == "success":
        save_incremental_result(result, output_dir)
        print(f"\n‚úÖ [{result['item']}] Complete")
        print(f"   Files: {output_dir}/")
```

**Benefits:**
- Get partial results even if some tasks fail
- Can start analyzing data while batch still running
- Better user experience for long-running batches

---

## Implementation Priority

### Phase 1: Critical (Implement Now)
1. **Pre-flight validation** - Highest ROI, catches errors in minutes
2. **Real-time monitoring** - Know within 5-10 min if workers are working

### Phase 2: Important (Next Sprint)
3. **Worker logging** - Essential for debugging failures
4. **Fail-fast mode** - Avoid wasting time on broken batches

### Phase 3: Nice-to-Have (Future)
5. **Incremental results** - Better UX for long batches

---

## Example: Improved User Experience

### Current Experience (Bad)
```
--- Batch Dispatch Initiated ---
Run ID: 20260209_141215
Items: 38
[... 2 hours of silence ...]

--- Batch Job Complete ---
Success: 0/38
Timeouts: 38
```

### Improved Experience (Good)
```
--- Batch Dispatch Initiated ---
Run ID: 20260209_153456
Items: 38

üß™ PRE-FLIGHT: Testing catalog-scraper on first item...
   Running: Anthropic (https://www.anthropic.com/learn)
   [... 3 minutes ...]
‚úÖ PRE-FLIGHT PASSED: Skill working correctly
   Output: ['provider', 'title', 'url', 'description', ...]
   Generated: anthropic_catalog.json, anthropic_catalog.xlsx

üöÄ Launching full batch (38 items, 5 workers)

[0] Starting: Anthropic
[1] Starting: SANS
[2] Starting: DataDog
[3] Starting: Weights & Biases
[4] Starting: McKinsey

üìä PROGRESS (15:39:22) - 2 minutes elapsed
   Completed: 0/38
   Has output: 3 tasks (Anthropic, SANS, DataDog)
   Workers active: 5/5

‚úÖ [Anthropic] Complete (4m 32s)
   Files: batch_results_20260209_153456/anthropic_catalog.*

üìä PROGRESS (15:41:22) - 4 minutes elapsed
   Completed: 3/38 (8%)
   Has output: 5 tasks
   Est. completion: 16:52

[... continues with regular updates ...]

--- Batch Job Complete ---
Success: 35/38 (92%)
Errors: 2
Timeouts: 1
```

---

## Code Changes Required

**Files to modify:**
- `batch_runner.py` - Add validation, monitoring, logging
- `SKILL.md` - Document new flags and behavior

**New CLI flags:**
```bash
--skip-preflight       # Skip pre-flight validation
--fail-fast=N          # Abort if first N tasks fail
--monitor-interval=120 # Progress update frequency (seconds)
--no-worker-logs       # Disable worker stdout/stderr logging
```

**Backward compatibility:**
- All new features are opt-in or have sensible defaults
- Existing scripts continue to work unchanged

---

## Success Metrics

After implementing these improvements:
- ‚úÖ Detect configuration errors within 20 minutes (vs 2+ hours)
- ‚úÖ Know if workers are producing output within 5 minutes
- ‚úÖ Can debug failures without re-running full batches
- ‚úÖ Don't waste time on systematically broken workflows
- ‚úÖ Get partial results from successful tasks

---

## Related Issues

- **Timeout tuning:** How to pick the right timeout value? (auto-detect based on first task?)
- **Cost monitoring:** Track API costs per task for budget visibility
- **Retry logic:** Auto-retry failed tasks with exponential backoff?
- **Parallel scaling:** Auto-scale max-workers based on system resources?

---

## Notes

- These improvements are generalizable to any batch-dispatch workflow
- Most valuable for long-running tasks (scraping, video processing, etc.)
- Pre-flight validation alone would have saved ~2 hours today
- Consider making monitoring the default, with --quiet to disable
