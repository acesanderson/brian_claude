from __future__ import annotations

import json
import re
import time
from datetime import datetime

import pandas as pd
import requests
from bs4 import BeautifulSoup
from unidecode import unidecode


def clean_to_ascii(text: str | None) -> str:
    """Convert Unicode text to clean ASCII."""
    if not text:
        return ""
    return unidecode(str(text))


def scrape_catalog(catalog_url: str) -> list[dict]:
    """
    Scrape Google Cloud Skills Boost catalog.

    Strategy: The catalog page uses JavaScript to load course data.
    We need to find the API endpoint or embedded JSON data.
    """
    print(f"Fetching catalog page: {catalog_url}")

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }

    response = requests.get(catalog_url, headers=headers)
    response.raise_for_status()

    # Save the HTML for inspection
    with open("catalog_page.html", "w", encoding="utf-8") as f:
        f.write(response.text)

    soup = BeautifulSoup(response.text, "html.parser")

    # Strategy 1: Look for JSON in script tags
    print("\nSearching for embedded JSON data...")
    scripts = soup.find_all("script")
    courses = []

    for script in scripts:
        if script.string:
            # Look for course data patterns
            if "catalog" in script.string.lower() or "courses" in script.string.lower():
                # Try to extract JSON
                json_match = re.search(r'({[\s\S]*?"title"[\s\S]*?})', script.string)
                if json_match:
                    print("Found potential course data in script tag")

    # Strategy 2: Try the API endpoint directly
    # Google Cloud Skills Boost likely has an API endpoint like /api/catalog or similar
    api_endpoints = [
        "https://www.cloudskillsboost.google/catalog.json",
        "https://www.cloudskillsboost.google/api/catalog",
        "https://www.cloudskillsboost.google/api/v1/catalog",
    ]

    for endpoint in api_endpoints:
        try:
            print(f"\nTrying API endpoint: {endpoint}")
            api_response = requests.get(endpoint, headers=headers)
            if api_response.status_code == 200:
                data = api_response.json()
                print(f"✓ Found data at {endpoint}")
                return data
        except Exception as e:
            print(f"  ✗ Failed: {e}")

    # Strategy 3: Parse the HTML for rendered courses (if any)
    print("\nSearching for server-rendered course cards...")
    course_cards = soup.find_all(class_=re.compile(r"(course|catalog|card|item)", re.I))
    print(f"Found {len(course_cards)} potential course elements")

    # If we can't find data, we need to document this
    if not courses:
        print("\n⚠️  Could not extract course data from initial page load.")
        print("This catalog likely requires:")
        print("  - JavaScript execution (Selenium/Playwright)")
        print("  - API endpoint discovery (Network tab inspection)")
        print("  - Authentication/session handling")

        # Try to extract what we can from the HTML
        return extract_from_html(soup)

    return courses


def extract_from_html(soup: BeautifulSoup) -> list[dict]:
    """
    Attempt to extract courses from HTML structure.
    This is a fallback if JSON/API not available.
    """
    courses = []

    # Look for common patterns
    # Google Cloud may use specific class names or data attributes
    potential_courses = soup.find_all(["article", "div"], class_=re.compile(r"(course|catalog-item|card)", re.I))

    print(f"\nAttempting to parse {len(potential_courses)} potential course elements...")

    for element in potential_courses[:5]:  # Sample first 5
        try:
            # Try to extract basic info
            title_elem = element.find(["h1", "h2", "h3", "h4"])
            link_elem = element.find("a")

            if title_elem and link_elem:
                course = {
                    "title": title_elem.get_text(strip=True),
                    "url": link_elem.get("href", ""),
                    "html_sample": str(element)[:200]
                }
                courses.append(course)
        except Exception as e:
            continue

    return courses


def transform_course_data(raw_courses: list[dict], provider: str, catalog_url: str) -> list[dict]:
    """Transform scraped data to standard format."""
    courses_data = []

    for course in raw_courses:
        transformed = {
            "provider": provider,
            "title": course.get("title", ""),
            "url": course.get("url", catalog_url),
            "description": course.get("description", ""),
            "duration": course.get("duration", ""),
            "level": course.get("level", ""),
            "format": course.get("format", "Self-Paced"),
            "price": course.get("price", "Free"),
            "category": course.get("category", ""),
            "credential_type": course.get("credential_type"),
            "date_scraped": datetime.now().strftime("%Y-%m-%d")
        }
        courses_data.append(transformed)

    return courses_data


def export_catalog_data(courses_data: list[dict], provider_slug: str) -> dict[str, str]:
    """Export to JSON and XLSX with validation."""
    df = pd.DataFrame(courses_data)

    # Column order
    column_order = [
        "provider", "title", "url", "description", "duration",
        "level", "format", "price", "category", "credential_type",
        "date_scraped"
    ]
    existing_cols = [col for col in column_order if col in df.columns]
    df = df[existing_cols]

    # VALIDATE TITLE QUALITY
    if "title" in df.columns and len(df) > 0:
        df["title_length"] = df["title"].str.len()
        avg_title_length = df["title_length"].mean()
        long_titles = df[df["title_length"] > 150]

        print(f"\n⚠️  TITLE QUALITY CHECK:")
        print(f"  Average title length: {avg_title_length:.0f} characters")

        if len(long_titles) > 0:
            print(f"  ❌ WARNING: {len(long_titles)} titles are >150 characters")
            print(f"     Long titles suggest description text is bleeding into title field.")

        df = df.drop(columns=["title_length"])

    # VALIDATE DESCRIPTION QUALITY
    if "description" in df.columns and len(df) > 0:
        total_descs = len(df)
        unique_descs = df["description"].nunique()
        uniqueness_pct = (unique_descs / total_descs * 100) if total_descs > 0 else 0

        print(f"\n⚠️  DESCRIPTION QUALITY CHECK:")
        print(f"  Total courses: {total_descs}")
        print(f"  Unique descriptions: {unique_descs}")
        print(f"  Uniqueness: {uniqueness_pct:.1f}%")

        if uniqueness_pct < 90:
            print(f"  ❌ WARNING: Low description uniqueness ({uniqueness_pct:.1f}%)")

    # 1. JSON
    json_filename = f"{provider_slug}_catalog.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(courses_data, f, indent=2, ensure_ascii=False)

    # 2. XLSX
    xlsx_filename = f"{provider_slug}_catalog.xlsx"
    text_columns = df.select_dtypes(include=["object"]).columns
    for col in text_columns:
        df[col] = df[col].apply(clean_to_ascii)
    df.to_excel(xlsx_filename, index=False, engine="openpyxl")

    print(f"\n✓ Generated JSON: {json_filename}")
    print(f"✓ Generated XLSX: {xlsx_filename}")

    return {"json": json_filename, "xlsx": xlsx_filename}


def generate_report(courses_data: list[dict], provider: str, provider_slug: str, catalog_url: str) -> str:
    """Generate markdown report."""
    report_filename = f"{provider_slug}_report.md"

    report = f"""# {provider} Catalog Scraping Report

**Date**: {datetime.now().strftime("%Y-%m-%d")}
**URL**: {catalog_url}
**Total Courses**: {len(courses_data)}

## Architecture
- Type: JavaScript-heavy catalog (requires further investigation)
- Data Source: API endpoint or embedded JSON (needs discovery)
- Obstacles: Dynamic content loading

## Extraction Method
Initial reconnaissance phase. Full extraction requires:
1. Network tab inspection to find API endpoints
2. Or browser automation (Selenium/Playwright) for JavaScript execution
3. Session/authentication handling if needed

## Data Quality
- Courses found: {len(courses_data)}

## Limitations
- Initial HTML provides minimal data
- Requires JavaScript execution or API endpoint discovery
- May need authentication for full catalog access

## Recommendations
1. Inspect browser Network tab while loading catalog
2. Look for API calls to `/api/catalog` or similar
3. Consider using Selenium if API not accessible
4. Check if authentication provides more data

## Next Steps
This is a reconnaissance phase. To proceed:
- Use browser developer tools to find data source
- Implement appropriate extraction strategy
- Re-run scraper with discovered method
"""

    with open(report_filename, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"✓ Generated report: {report_filename}")
    return report_filename


def main():
    """Main execution function."""
    provider = "Google Cloud"
    provider_slug = "google_cloud"
    catalog_url = "https://www.cloudskillsboost.google/catalog"

    print(f"\n{'='*60}")
    print(f"Google Cloud Skills Boost Catalog Scraper")
    print(f"{'='*60}\n")

    # Scrape catalog
    raw_courses = scrape_catalog(catalog_url)

    if not raw_courses:
        print("\n⚠️  No courses found in initial scrape.")
        print("This requires deeper investigation of the site's data loading mechanism.")

        # Create minimal report documenting the issue
        generate_report([], provider, provider_slug, catalog_url)
        return

    # Transform to standard format
    print("\nTransforming data to standard format...")
    courses_data = transform_course_data(raw_courses, provider, catalog_url)
    print(f"✓ Transformed {len(courses_data)} courses")

    # Export data
    print("\nExporting data...")
    files = export_catalog_data(courses_data, provider_slug)

    # Generate report
    print("\nGenerating report...")
    report_file = generate_report(courses_data, provider, provider_slug, catalog_url)

    # Summary
    print(f"\n{'='*60}")
    print("SCRAPING COMPLETE")
    print(f"{'='*60}")
    print(f"\n✓ Scraped {len(courses_data):,} courses from {provider}")
    print(f"\nARTIFACTS GENERATED:")
    print(f"  ✓ JSON:   {files['json']}")
    print(f"  ✓ XLSX:   {files['xlsx']}")
    print(f"  ✓ Report: {report_file}")
    print()


if __name__ == "__main__":
    main()
