from __future__ import annotations

import json
import re
import time
from datetime import datetime

import pandas as pd
import requests
from bs4 import BeautifulSoup

def get_page_content(url: str) -> str:
    """Fetch page with proper headers."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    }
    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()
    return response.text

def extract_courses_from_catalog(html: str) -> list[dict]:
    """Extract course/path listings from main catalog page."""
    soup = BeautifulSoup(html, 'html.parser')
    courses = []

    # Find all links to courses/paths/plans
    course_links = soup.find_all('a', href=lambda x: x and ('/course' in x or '/path' in x or '/plan' in x))
    print(f"Found {len(course_links)} course/path/plan links\n")

    seen_urls = set()

    for link in course_links:
        href = link.get('href', '')
        if not href or href in seen_urls:
            continue

        # Build full URL
        url = href if href.startswith('http') else 'https://training.zendesk.com' + href

        # Skip login/auth links
        if 'login' in href or 'auth' in href or href.startswith('#'):
            continue

        seen_urls.add(href)

        # Get title from link text
        title = link.get_text(strip=True)
        if not title or len(title) < 3:
            continue

        # Try to get description from nearby elements
        description = ''
        parent = link.find_parent(['div', 'article', 'section'])
        if parent:
            # Look for description text
            desc_elem = parent.find(['p', 'div'], class_=re.compile(r'description|summary|intro', re.I))
            if desc_elem:
                description = desc_elem.get_text(strip=True)
            elif parent.get_text():
                # Get text that's not the title
                full_text = parent.get_text(strip=True)
                if len(full_text) > len(title):
                    description = full_text.replace(title, '').strip()

        # Determine type from URL
        course_type = 'Learning Path' if '/path/' in href else 'Plan' if '/plan/' in href else 'Course'

        # Extract any visible metadata
        duration = ''
        level = ''
        category = ''

        course = {
            'provider': 'Zendesk',
            'title': title,
            'url': url,
            'description': description[:500] if description else '',  # Limit description length
            'duration': duration,
            'level': level,
            'format': 'On-Demand',
            'price': 'Free',  # Zendesk training is free for customers
            'category': category,
            'learning_path': course_type,
            'date_scraped': datetime.now().strftime('%Y-%m-%d')
        }

        courses.append(course)
        print(f"Extracted: {title[:60]}")

    return courses

def enhance_course_details(course: dict) -> dict:
    """Optionally fetch individual course page for more details."""
    # This could be expanded to visit each course page
    # For now, keeping it simple with catalog-level data
    return course

def scrape_zendesk_catalog() -> list[dict]:
    """Main scraping function."""
    print("=" * 60)
    print("Zendesk Training Catalog Scraper")
    print("=" * 60)
    print("URL: https://training.zendesk.com\n")

    html = get_page_content('https://training.zendesk.com')
    courses = extract_courses_from_catalog(html)

    print(f"\n{'='*60}")
    print(f"Total unique courses/paths found: {len(courses)}")
    print(f"{'='*60}\n")

    return courses

def export_data(courses: list[dict], provider_slug: str) -> dict[str, str]:
    """Export to JSON and CSV."""
    # JSON
    json_filename = f"{provider_slug}_catalog.json"
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(courses, f, indent=2, ensure_ascii=False)

    # CSV
    df = pd.DataFrame(courses)
    column_order = [
        'provider', 'title', 'url', 'description', 'duration',
        'level', 'format', 'price', 'category', 'learning_path', 'date_scraped'
    ]
    df = df[column_order]
    csv_filename = f"{provider_slug}_catalog.csv"
    df.to_csv(csv_filename, index=False, encoding='utf-8')

    print(f"✓ JSON saved: {json_filename}")
    print(f"✓ CSV saved: {csv_filename}")

    return {"json": json_filename, "csv": csv_filename}

def generate_report(courses: list[dict], provider_slug: str) -> str:
    """Generate markdown report."""
    df = pd.DataFrame(courses)

    # Calculate completeness
    total = len(courses)
    completeness = {
        'title': (df['title'].notna() & (df['title'] != '')).sum() / total * 100 if total > 0 else 0,
        'description': (df['description'].notna() & (df['description'] != '')).sum() / total * 100 if total > 0 else 0,
        'url': (df['url'].notna() & (df['url'] != '')).sum() / total * 100 if total > 0 else 0,
    }

    # Type breakdown
    type_counts = df['learning_path'].value_counts() if 'learning_path' in df else {}

    report = f"""# Zendesk Training Catalog Scraping Report

**Date**: {datetime.now().strftime('%Y-%m-%d')}
**URL**: https://training.zendesk.com
**Total Courses/Paths**: {len(courses)}

## Architecture
- **Type**: Single Page Catalog
- **Platform**: Skilljar (SaaS learning platform)
- **Data Source**: Server-rendered HTML
- **Obstacles**: CloudFront protection (bypassed with proper User-Agent headers)

## Extraction Method
Extracted course and learning path links from the main catalog page. The site organizes content into:
- **Courses**: Individual training modules
- **Learning Paths**: Curated sequences of courses
- **Plans**: Training packages

Used BeautifulSoup to parse HTML and extract all links with `/course`, `/path`, or `/plan` in the URL.

## Data Quality
- Title: {completeness['title']:.1f}% complete
- Description: {completeness['description']:.1f}% complete
- URL: {completeness['url']:.1f}% complete

## Content Type Breakdown
"""

    for content_type, count in type_counts.items():
        report += f"- {content_type}: {count}\n"

    report += f"""
## Limitations
- Duration and level information not consistently available on catalog page
- Descriptions are abbreviated; full details would require visiting individual course pages
- Multi-language versions of same course appear as separate entries
- Some metadata (prerequisites, learning objectives) only available on individual course pages

## Recommendations
- **Licensing Consideration**: Zendesk's content is product-specific (Zendesk CRM platform), limiting broad appeal
- **Target Audience**: Best suited for Zendesk customers and administrators
- **Content Depth**: Mix of beginner admin guides and advanced configuration topics
- **Multi-language**: Extensive localization (Japanese, Spanish, French, German, Portuguese)
- **Structure**: Learning paths provide good scaffolding for skill progression

## Sample Courses

"""

    # Add sample courses
    for i, course in enumerate(courses[:10], 1):
        report += f"""### {i}. {course['title']}
- **Type**: {course.get('learning_path', 'Course')}
- **URL**: {course['url']}
- **Description**: {course['description'][:200] if course['description'] else 'No description available'}{'...' if len(course.get('description', '')) > 200 else ''}

"""

    report_filename = f"{provider_slug}_report.md"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"✓ Report saved: {report_filename}")
    return report_filename

if __name__ == '__main__':
    courses = scrape_zendesk_catalog()

    if courses:
        provider_slug = 'zendesk'
        files = export_data(courses, provider_slug)
        report_file = generate_report(courses, provider_slug)

        print(f"\n{'='*60}")
        print("SCRAPING COMPLETE - ARTIFACTS GENERATED")
        print(f"{'='*60}")
        print(f"✓ {files['json']}")
        print(f"✓ {files['csv']}")
        print(f"✓ {report_file}")

        print(f"\n{'='*60}")
        print("PREVIEW - First 5 Courses")
        print(f"{'='*60}")
        for i, course in enumerate(courses[:5], 1):
            print(f"\n{i}. {course['title']}")
            print(f"   Type: {course.get('learning_path', 'Course')}")
            print(f"   URL: {course['url']}")
    else:
        print("\n❌ No courses found. Check the HTML structure.")
