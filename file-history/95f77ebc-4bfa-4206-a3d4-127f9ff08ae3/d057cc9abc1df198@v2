from __future__ import annotations

import json
import re
import time
from datetime import datetime

import pandas as pd
import requests
from bs4 import BeautifulSoup


def extract_courses_from_js_data(html_content: str, base_url: str, category_name: str) -> list[dict]:
    """Extract course data from JavaScript skilljarCatalogPage object using regex."""
    courses = []

    # Find the catalog_page_items array
    match = re.search(r'catalog_page_items:\s*\[(.*?)\]', html_content, re.DOTALL)
    if not match:
        return courses

    items_text = match.group(1)

    # Extract individual course objects using regex
    course_pattern = r'\{[^}]*id:\s*[\'"]([^\'"]+)[\'"][^}]*title:\s*[\'"]([^\'"]+)[\'"][^}]*slug:\s*[\'"]([^\'"]+)[\'"][^}]*type:\s*[\'"]([^\'"]+)[\'"][^}]*\}'

    for match in re.finditer(course_pattern, items_text, re.DOTALL):
        course_id, title, slug, item_type = match.groups()

        # Skip non-course items
        if item_type != "COURSE":
            continue

        # Decode unicode escapes in title and slug
        title = title.replace(r'\u002D', '-').replace(r'\u002D\u002D', '--')
        slug = slug.replace(r'\u002D', '-')

        course_url = f"{base_url}{slug}"

        course = {
            "provider": "Procore Learning",
            "title": title,
            "url": course_url,
            "description": "",  # Not in catalog_page_items
            "duration": "",  # Not in catalog_page_items
            "level": "",
            "format": "Self-Paced",
            "price": "Free",
            "category": category_name,
            "instructor": "",
            "date_scraped": datetime.now().strftime("%Y-%m-%d")
        }

        courses.append(course)

    return courses


def scrape_procore_catalog() -> list[dict]:
    """Scrape course catalog from Procore Learning."""
    base_url = "https://learn.procore.com/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }

    # Pages to scrape
    pages = [
        ("product-certifications", "Product Education"),
        ("procore-tool-training", "Procore Tool Training"),
        ("continuing-education", "Construction Education"),
        ("project-management", "Project Management"),
        ("safety-in-construction", "Safety"),
        ("construction-fundamentals", "Construction Fundamentals"),
        ("technology-and-innovation", "Tech and Innovation"),
        ("procore-safety-qualified", "Procore Safety Qualified"),
        ("data-in-construction-category", "Data in Construction"),
        ("workforce-development", "Workforce Development"),
        ("project-manager-courses", "Project Manager Courses"),
        ("field-operations-courses", "Field Operations Courses"),
        ("safety-manager-courses", "Safety Manager Courses"),
        ("preconstruction-pro-courses", "Preconstruction Pro Courses"),
        ("financial-admins-courses", "Financial Admins Courses"),
        ("ai-in-construction", "AI in Construction"),
    ]

    all_courses = []

    for page_slug, page_name in pages:
        url = f"{base_url}page/{page_slug}"
        print(f"Fetching {page_name} ({url})...")

        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()

            courses = extract_courses_from_js_data(response.text, base_url, page_name)
            print(f"Found {len(courses)} courses in {page_name}")

            all_courses.extend(courses)

            # Be respectful - wait between requests
            time.sleep(1)

        except Exception as e:
            print(f"Error fetching {page_name}: {e}")
            continue

    # Remove duplicates based on URL
    seen_urls = set()
    unique_courses = []
    for course in all_courses:
        if course["url"] not in seen_urls:
            seen_urls.add(course["url"])
            unique_courses.append(course)

    print(f"\nTotal unique courses extracted: {len(unique_courses)}")
    return unique_courses


def export_catalog_data(courses_data: list[dict], provider_slug: str) -> dict[str, str]:
    """Export to JSON and CSV."""
    df = pd.DataFrame(courses_data)

    # Column order
    column_order = [
        "provider", "title", "url", "description", "duration",
        "level", "format", "price", "category", "instructor",
        "date_scraped"
    ]
    existing_cols = [col for col in column_order if col in df.columns]
    df = df[existing_cols]

    # 1. JSON
    json_filename = f"{provider_slug}_catalog.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(courses_data, f, indent=2, ensure_ascii=False)
    print(f"✓ Saved {json_filename}")

    # 2. CSV
    csv_filename = f"{provider_slug}_catalog.csv"
    df.to_csv(csv_filename, index=False)
    print(f"✓ Saved {csv_filename}")

    return {"json": json_filename, "csv": csv_filename}


def generate_report(courses: list[dict], provider_slug: str) -> str:
    """Generate markdown report."""
    report_filename = f"{provider_slug}_report.md"

    df = pd.DataFrame(courses)
    total_courses = len(courses)

    # Calculate completeness
    completeness = {
        "Title": (df["title"].notna() & (df["title"] != "")).sum() / total_courses * 100,
        "Description": (df["description"].notna() & (df["description"] != "")).sum() / total_courses * 100,
        "Duration": (df["duration"].notna() & (df["duration"] != "")).sum() / total_courses * 100,
        "Category": (df["category"].notna() & (df["category"] != "")).sum() / total_courses * 100,
    }

    # Category breakdown
    category_counts = df["category"].value_counts().to_dict()

    report = f"""# Procore Learning Catalog Scraping Report

**Date**: {datetime.now().strftime("%Y-%m-%d")}
**URL**: https://learn.procore.com/
**Total Courses**: {total_courses}

## Architecture
- **Type**: Single page with JavaScript-enhanced carousels
- **Data Source**: Server-rendered HTML with course cards
- **Obstacles**: None (publicly accessible)
- **Extraction Method**: BeautifulSoup HTML parsing

## Extraction Method
Static HTML scraping using BeautifulSoup. Course cards are embedded directly in the page markup with consistent structure. Each card contains:
- Two h2 elements (category and title)
- One p element (description)
- Div element with duration information
- Anchor tag with course URL

The scraper extracts all course cards from the page in a single request.

## Data Quality
- **Title**: {completeness['Title']:.1f}% complete
- **Description**: {completeness['Description']:.1f}% complete
- **Duration**: {completeness['Duration']:.1f}% complete
- **Category**: {completeness['Category']:.1f}% complete
- **Level**: 0% complete (not available on listing page)
- **Price**: 100% inferred (platform is free)

## Category Breakdown
"""

    for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
        report += f"- {category}: {count} courses\n"

    report += f"""
## Limitations
- **Level information** not visible on the catalog listing page (would require visiting individual course pages)
- **Instructor information** not available
- **Duration format** varies (some show ranges like "1-2 hours", others show exact times)
- Course count on page may include duplicates in different carousel sections

## Recommendations
1. **For complete metadata**: Consider scraping individual course pages to extract level, prerequisites, and detailed learning objectives
2. **For de-duplication**: Check for duplicate URLs if courses appear in multiple sections
3. **For certification info**: Individual course pages likely contain certification details

## Sample Courses

"""

    # Add 5 sample courses
    for course in courses[:5]:
        report += f"""### {course['title']}
- **Category**: {course['category']}
- **Duration**: {course['duration']}
- **URL**: {course['url']}
- **Description**: {course['description'][:150]}...

"""

    with open(report_filename, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"✓ Saved {report_filename}")
    return report_filename


if __name__ == "__main__":
    print("=== Procore Learning Catalog Scraper ===\n")

    # Scrape courses
    courses = scrape_procore_catalog()

    if not courses:
        print("No courses found. Exiting.")
        exit(1)

    # Export data
    print("\nExporting data...")
    export_catalog_data(courses, "procore")

    # Generate report
    print("\nGenerating report...")
    generate_report(courses, "procore")

    print("\n=== Scraping Complete ===")
    print(f"✓ {len(courses)} courses extracted")
    print("✓ JSON, CSV, and report generated")
